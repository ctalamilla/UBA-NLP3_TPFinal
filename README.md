# RAG â€“ BoletÃ­n Oficial de Salta

Sistema RAG hÃ­brido (BM25 + Pinecone + LLM) para consultar el BoletÃ­n Oficial de Salta.  
Stack completo con orquestaciÃ³n en Airflow, almacenamiento en MinIO (S3), API en FastAPI y frontend en Streamlit.
A continuaciÃ³n un video explicativo.
[![Ver video](figures/image.png)](https://drive.google.com/file/d/1Wss4I4zJwlMydGc2-BnwthNEwQ-NOZz9/view?usp=drive_link)

---

## ğŸ“‹ Tabla de Contenidos

- [Arquitectura](#-arquitectura)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Requisitos](#-requisitos)
- [ConfiguraciÃ³n](#-configuraciÃ³n)
- [Despliegue](#-despliegue)
- [Uso](#-uso)
- [Estructura de Datos en MinIO](#-estructura-de-datos-en-minio)
- [Troubleshooting](#-troubleshooting)

---
## ğŸ“– IntroducciÃ³n

Este proyecto implementa un sistema RAG (Retrieval-Augmented Generation) de producciÃ³n para consultar el BoletÃ­n Oficial de la provincia de Salta, Argentina. El sistema permite realizar bÃºsquedas semÃ¡nticas sobre ordenanzas provinciales (OPs) y obtener respuestas contextualizadas generadas por LLMs.

### CaracterÃ­sticas Principales

- **RecuperaciÃ³n HÃ­brida**: Combina BM25 (bÃºsqueda lÃ©xica) + embeddings vectoriales (bÃºsqueda semÃ¡ntica) mediante Reciprocal Rank Fusion (RRF)
- **ClasificaciÃ³n AutomÃ¡tica**: Agente LLM que clasifica y extrae metadatos de cada OP
- **Chunking Inteligente**: SegmentaciÃ³n orientada a OPs para preservar contexto legal
- **VerificaciÃ³n de Respuestas**: Guardrails de entrada/salida para prevenir alucinaciones
- **MÃ©tricas de Calidad**: EvaluaciÃ³n automÃ¡tica con AP@k, nDCG@k, Recall@k y MRR
- **Pipeline Batch**: Procesamiento por lotes orquestado con Airflow
- **Inference API**: FastAPI con latencias <5s y costos monitoreados

### InspiraciÃ³n y MetodologÃ­a

Este proyecto sigue las mejores prÃ¡cticas del libro **"LLM Engineer's Handbook: Master the art of engineering large language models from concept to production"** de Paul Iusztin, Maxime Labonne y Julien Chaumond (Packt Publishing).

<div align="center">
  <img src="figures/fig3.jpeg" alt="LLM Engineer's Handbook" width="300"/>
</div>

El diseÃ±o del pipeline se basa en el patrÃ³n **Batch RAG Feature Pipeline** descrito en el libro, adaptado al caso especÃ­fico de documentos legales estructurados:

<div align="center">
  <img src="figures/fig2.png" alt="Batch RAG Feature Pipeline" width="700"/>
  <p><em>Figura: Pipeline RAG genÃ©rico (fuente: LLM Engineer's Handbook)</em></p>
</div>

**Adaptaciones clave al dominio legal:**
- **Raw Docs â†’ PDFs del BoletÃ­n Oficial** (fuente Ãºnica, formato consistente)
- **Clean â†’ ExtracciÃ³n y split por OP** (cada ordenanza provincial es una unidad lÃ³gica)
- **Metadata Enrichment â†’ ClasificaciÃ³n con LLM** (categorÃ­a, fechas, extractos relevantes)
- **Chunk â†’ SegmentaciÃ³n contextual** (mantiene coherencia dentro de cada OP)
- **Embed â†’ VectorizaciÃ³n con MiniLM** (modelo multilingÃ¼e optimizado para espaÃ±ol)
- **Dual Indexing â†’ BM25 + Pinecone** (recuperaciÃ³n hÃ­brida para mÃ¡xima cobertura)

---

## ğŸ”„ Pipeline de Ingesta

El flujo completo de datos desde el PDF original hasta los Ã­ndices de recuperaciÃ³n:

<div align="center">
  <img src="figures/fig1.png" alt="Pipeline de Ingesta - BoletÃ­n Oficial" width="900"/>
  <p><em>Figura: Arquitectura del pipeline de procesamiento e indexaciÃ³n</em></p>
</div>

### Etapas del Pipeline

1. **ExtracciÃ³n por OP** ğŸ“„
   - Input: PDF del BoletÃ­n Oficial (ej: `22044_2025-10-02.pdf`)
   - Output: TXT individuales por OP + metadata JSON
   - Storage: `s3://respaldo2/rag/text_op/` y `text_op_meta/`

2. **ClasificaciÃ³n AutomÃ¡tica** ğŸ§ 
   - Agent LLM analiza cada OP y extrae:
     - CategorÃ­a (edictos, licitaciones, resoluciones, etc.)
     - Entidades mencionadas (personas, empresas, expedientes)
     - Fechas relevantes
     - Resumen ejecutivo
   - Output: JSON enriquecido con clasificaciÃ³n
   - Storage: Mismo path que metadata, campo `classification`

3. **Chunking Contextual** âœ‚ï¸
   - SegmentaciÃ³n respetando lÃ­mites de OP
   - Cada chunk mantiene metadata del boletÃ­n y OP origen
   - Output: NDJSON con formato:
```json
 {"chunk_id": "22044::OP100128767::1", "text": "...", "metadata": {...}}
```
   - Storage: `s3://respaldo2/rag/chunks_op/2025/`
4. **IndexaciÃ³n Dual** ğŸ”
   - **BM25**: Ãndice lÃ©xico para bÃºsqueda por tÃ©rminos exactos
     - Storage: `s3://respaldo2/rag/models/2025/bm25.pkl`
   - **Pinecone**: Base vectorial para bÃºsqueda semÃ¡ntica
     - Index: `boletines-2025` (dimensiÃ³n: 384, MiniLM)
     - Namespace: `2025`

5. **EvaluaciÃ³n (Opcional)** ğŸ“Š
   - MÃ©tricas de ranking contra ground truth (qrels.csv)
   - Reportes en `s3://respaldo2/rag/metrics/`

---

## ğŸ— Arquitectura

El sistema se compone de **15 servicios** en Docker Compose:

### **OrquestaciÃ³n y Procesamiento (Airflow)**
- `postgres` - Base de datos PostgreSQL para Airflow
- `redis` - Cola de mensajes para CeleryExecutor
- `airflow-webserver` - UI de Airflow (puerto **8080**)
- `airflow-scheduler` - Planificador de DAGs
- `airflow-worker` - Worker de Celery para ejecutar tasks
- `airflow-triggerer` - Manejo de triggers asÃ­ncronos
- `airflow-init` - InicializaciÃ³n automÃ¡tica (DB, usuario admin)
- `airflow-cli` - CLI de Airflow (profile `debug`)
- `flower` - Monitor de Celery (puerto **5555**, profile `flower`)

### **Almacenamiento y ML**
- `minio` - S3-compatible storage (API: **9000**, Console: **9001**)
- `init-minio` - Crea bucket `respaldo2` automÃ¡ticamente
- `mlflow` - MLflow tracking server (puerto **5001**)

### **RAG y Frontend**
- `rag_api` - FastAPI con el pipeline RAG (puerto **8000**)
- `rag_notebook` - Jupyter notebook para desarrollo (puerto **8888**)
- `streamlit` - UI interactiva (puerto **8501**)

---

## ğŸ“ Estructura del Proyecto

```text
.
â”œâ”€ dags/
â”‚  â””â”€ Ingestion_pipeline.py        # DAG de Airflow que orquesta el flujo ETL/RAG completo.
â”‚                                  # Llama a tasks/* (extract > chunk > index > eval).
â”œâ”€ datalake/                        # Carpeta local (opcional) para staging; en prod se usa S3/MinIO.
â”œâ”€ docker-compose.yaml              # Levanta Airflow, FastAPI, Streamlit, MinIO, etc.
â”œâ”€ Dockerfile                       # Imagen base (raÃ­z) â€“ Ãºtil para notebooks o jobs sueltos.

â”œâ”€ fastapi_app/
â”‚  â”œâ”€ Dockerfile                    # Imagen del microservicio de consulta.
â”‚  â”œâ”€ main.py                       # Endpoints (/health, /ask, /query, /vector/query, /eval/*).
â”‚  â”œâ”€ performance.py                # Endpoints/Ãºtiles para medir latencias y throughput.
â”‚  â”œâ”€ pipeline.py                   # Pipeline RAG hÃ­brido (BM25 + Pinecone + RRF + LLM).
â”‚  â”œâ”€ requirements.txt
â”‚  â”œâ”€ s3_boto.py                    # Cliente S3/MinIO para leer bm25.pkl y NDJSON de chunks.
â”‚  â””â”€ vector_pinecone_api.py        # Helper de Pinecone (ensure_index, query con embeddings).

â”œâ”€ frontend_streamlit/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ requirements.txt
â”‚  â””â”€ streamlit_app.py              # UI simple para probar RAG (muestra chunk_id/source/score/â€¦).

â”œâ”€ mlflow/
â”‚  â””â”€ artifacts/                    # Artefactos de experimentos (si usÃ¡s MLflow localmente).
â”œâ”€ mlruns/                           # Metadatos de MLflow (experimentos, runs).

â”œâ”€ notebooks/
â”‚  â””â”€ pipeline_rag.ipynb            # Notebook de prototipado: chunking, fusiÃ³n RRF, pruebas locales.

â”œâ”€ plugins/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ tasks/                        # â€œLibrerÃ­a de tareasâ€ usada por Airflow y scripts.
â”‚     â”œâ”€ agent_classifier.py        # Clasificador LLM/heurÃ­stico por OP (categorÃ­a, extraÃ­dos).
â”‚     â”œâ”€ agente_verificador.py      # Verificaciones/guardrails con LLM (si aplica).
â”‚     â”œâ”€ bm25_build_task.py         # Construye Ã­ndice BM25 (bm25.pkl) desde NDJSON.
â”‚     â”œâ”€ bm25_dump_docids_task.py   # Exporta mapping de doc_ids (debug/diagnÃ³stico).
â”‚     â”œâ”€ bm25_index.py              # ImplementaciÃ³n BM25Index (search, doc_ids, persistencia).
â”‚     â”œâ”€ bm25_query_task.py         # Consulta de prueba contra el BM25.
â”‚     â”œâ”€ chunk_from_txt_task_op.py  # Chunker â€œOP-firstâ€: agrupa por boletÃ­n/op y emite NDJSON.
â”‚     â”œâ”€ chunk_from_txt_task.py     # Chunker â€œplainâ€: un NDJSON por PDF base (modo legacy).
â”‚     â”œâ”€ classify_chunks_agent_task.py # Clasifica chunks ya generados (etiquetas).
â”‚     â”œâ”€ classify_op_texts_task.py  # Extrae + clasifica cada OP desde TXT crudo.
â”‚     â”œâ”€ documents.py               # Clase Document + chunk_text (lÃ³gica de segmentaciÃ³n).
â”‚     â”œâ”€ eval_bm25_task.py          # MÃ©tricas sobre BM25 (recall@k, mrr, etc.).
â”‚     â”œâ”€ eval_fusion_task.py        # Eval de fusiÃ³n RRF (AP@k, nDCG@k, Recall@k, MRR).
â”‚     â”œâ”€ extract_texts_by_op_task.py# Crea text_op/ y text_op_meta/ desde PDFs o fuentes.
â”‚     â”œâ”€ fusion_rrf_task.py         # Ejecuta RRF (BM25 + vector) y guarda resultados en rag/fusion/.
â”‚     â”œâ”€ fusion.py                  # ImplementaciÃ³n pura de RRF (funciÃ³n rrf_combine).
â”‚     â”œâ”€ guardrail_task.py          # DetecciÃ³n de prompt-injection/ruido en chunks.
â”‚     â”œâ”€ io_utils.py                # Utilidades de IO locales.
â”‚     â”œâ”€ loader_pdfs.py             # PDFâ†’Document (limpieza, normalizaciÃ³n, dehyphen, etc.).
â”‚     â”œâ”€ make_qrels_task.py         # Construye qrels.csv (ground truth) para evaluaciÃ³n.
â”‚     â”œâ”€ metrics.py                 # MÃ©tricas de ranking (AP, nDCG, Recall, MRRâ€¦).
â”‚     â”œâ”€ pinecone_query_task.py     # Consulta vectorial de prueba (top_k) desde Airflow/script.
â”‚     â”œâ”€ pinecone_upsert_op_task.py # Upsert a Pinecone desde NDJSON â€œOP-firstâ€ (con metadatos extra).
â”‚     â”œâ”€ pinecone_upsert_task.py    # Upsert a Pinecone desde NDJSON legacy.
â”‚     â”œâ”€ procesamiento_utils.py     # Limpieza, normalizaciÃ³n, helpers de texto.
â”‚     â”œâ”€ qrels_utils.py             # Lectura/parseo de qrels.
â”‚     â”œâ”€ s3_utilities.py            # Listar/leer/subir a S3 (read_text, upload_json, list_keysâ€¦).
â”‚     â”œâ”€ s3_utils.py                # Compat/atajos S3 (antiguo).
â”‚     â”œâ”€ text_task.py               # Tareas miscelÃ¡neas de texto.
â”‚     â”œâ”€ utils_op_split.py          # Split de documentos en OPs (nombres, patrones).
â”‚     â”œâ”€ vector_pinecone_op.py      # Cliente Pinecone orientado a OP (IDs, namespaces).
â”‚     â””â”€ vector_pinecone.py         # Cliente Pinecone genÃ©rico (ensure_index, upsert, query).

â”œâ”€ rag_notebook/
â”‚  â”œâ”€ Dockerfile
â”‚  â””â”€ requirements.txt              # Entorno liviano para reproducir el notebook.

â””â”€ README.md                        # CÃ³mo correr, variables de entorno, flujo E2E.

```

---

## âœ… Requisitos

- **Docker** + **Docker Compose** v2.x
- **(Opcional)** `jq` para formatear respuestas JSON en terminal

---

## âš™ï¸ ConfiguraciÃ³n

### 1. Crear archivo `.env`

Crea un archivo `.env` en la raÃ­z del proyecto con las siguientes variables:

```bash
# ========================================
# MinIO / S3
# ========================================
S3_ENDPOINT_URL=http://minio:9000                    # URL interna (desde containers)
S3_ENDPOINT_URL_HOST=http://localhost:9000           # URL desde el host
AWS_ACCESS_KEY_ID=minio_admin                        # âš ï¸ Usuario MinIO (ver docker-compose)
AWS_SECRET_ACCESS_KEY=minio_admin                    # âš ï¸ Password MinIO
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET=respaldo2                                  # Bucket creado automÃ¡ticamente
S3_USE_SSL=false

# ========================================
# OpenAI
# ========================================
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXXXXXXXXXX     # âš ï¸ OBLIGATORIO
OPENAI_MODEL=gpt-4o-mini                            # Modelo por defecto
OPENAI_GUARD_MODEL=gpt-4o-mini                      # Guardrail de entrada
OPENAI_SUMMARY_MODEL=gpt-4o-mini                    # Resumen de contexto
OPENAI_ANSWER_MODEL=gpt-4o-mini                     # GeneraciÃ³n de respuesta
OPENAI_OUT_GUARD_MODEL=gpt-4o-mini                  # VerificaciÃ³n de salida

# ========================================
# Pinecone
# ========================================
PINECONE_API_KEY=pcsk_XXXXXXXX                      # âš ï¸ OBLIGATORIO
PINECONE_CLOUD=aws                                  # o 'gcp'
PINECONE_REGION=us-east-1                           # RegiÃ³n del Ã­ndice
PINECONE_INDEX=boletines-2025                       # Nombre del Ã­ndice
PINECONE_NAMESPACE=2025                             # Namespace (opcional)
PINECONE_ENVIRONMENT=us-east1-gcp                   # Solo si usÃ¡s pod-based

# ========================================
# Embeddings
# ========================================
EMB_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  # âš ï¸ DimensiÃ³n: 384

# ========================================
# Rutas en S3 (generadas por Airflow)
# ========================================
BM25_MODEL_KEY=rag/models/2025/bm25.pkl            # Ãndice BM25 serializado
CHUNKS_PREFIX=rag/chunks_op/2025/                  # NDJSON de chunks por boletÃ­n

# ========================================
# Airflow (usuarios admin)
# ========================================
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow

# ========================================
# Puertos (informativo)
# ========================================
# Airflow:     http://localhost:8080
# FastAPI:     http://localhost:8000
# Streamlit:   http://localhost:8501
# MinIO API:   http://localhost:9000
# MinIO UI:    http://localhost:9001
# MLflow:      http://localhost:5001
# Jupyter:     http://localhost:8888
# Flower:      http://localhost:5555 (con --profile flower)
```

> **âš ï¸ IMPORTANTE**: Las credenciales de MinIO son `minio_admin` / `minio_admin` (definidas en `docker-compose.yaml`). El servicio `init-minio` crea el bucket `respaldo2` automÃ¡ticamente al iniciar.

---

## ğŸš€ Despliegue

### OpciÃ³n 1: Levantar todo el stack

```bash
docker compose up -d --build
```

**Servicios disponibles:**

| Servicio | URL | Credenciales |
|----------|-----|--------------|
| **Airflow UI** | http://localhost:8080 | `airflow` / `airflow` |
| **FastAPI** | http://localhost:8000 | - |
| **Streamlit** | http://localhost:8501 | - |
| **MinIO Console** | http://localhost:9001 | `minio_admin` / `minio_admin` |
| **MLflow** | http://localhost:5001 | - |
| **Jupyter Notebook** | http://localhost:8888 | Sin token (dev only) |

### OpciÃ³n 2: Levantar servicios selectivos

```bash
# Solo infraestructura bÃ¡sica
docker compose up -d postgres redis minio

# Airflow + API + UI
docker compose up -d airflow-webserver airflow-scheduler airflow-worker rag_api streamlit

# Con Flower (monitor de Celery)
docker compose --profile flower up -d
```

---

## ğŸ¯ Uso

### 1. **Inicializar Airflow** (automÃ¡tico)

El servicio `airflow-init` se ejecuta automÃ¡ticamente y:
- Inicializa la base de datos
- Crea el usuario admin (`airflow` / `airflow`)
- Configura permisos de carpetas

**No se requiere ningÃºn comando manual.**

### 2. **Configurar conexiÃ³n MinIO en Airflow**

1. Acceder a **http://localhost:8080** (user: `airflow`, pass: `airflow`)
2. Ir a **Admin â†’ Connections**
3. Crear/editar conexiÃ³n:
   - **Conn Id**: `minio_s3`
   - **Conn Type**: `Amazon Web Services`
   - **Extra** (JSON):
     ```json
     {
       "endpoint_url": "http://minio:9000",
       "region_name": "us-east-1",
       "aws_access_key_id": "minio_admin",
       "aws_secret_access_key": "minio_admin"
     }
     ```

> **Nota**: El docker-compose define `AIRFLOW_CONN_MINIO_S3` en variables de entorno, por lo que esta conexiÃ³n puede estar pre-configurada.

### 3. **Ejecutar el pipeline de ingesta**

1. En Airflow UI, habilitar el DAG `Ingestion_pipeline`
2. Hacer clic en "Trigger DAG"
3. El pipeline ejecutarÃ¡:
   - **ExtracciÃ³n**: PDF â†’ TXT por OP
   - **Chunking**: TXT â†’ NDJSON con metadatos
   - **IndexaciÃ³n**: ConstrucciÃ³n de `bm25.pkl` + upsert a Pinecone
   - **EvaluaciÃ³n**: MÃ©tricas de ranking (opcional)

**Resultado**: Los archivos `bm25.pkl` y `*.ndjson` se subirÃ¡n a MinIO en `s3://respaldo2/rag/`.

---

## ğŸ” Consultar la API RAG

### Health check

```bash
curl -s http://localhost:8000/health | jq
```

### Consulta simple (GET)

```bash
curl -s "http://localhost:8000/ask?q=CASTRO,%20MARIA%20EUGENIA%20en%20que%20OP%20se%20menciona" | jq .answer
```

### Consulta completa con mÃ©tricas (POST)

```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "CASTRO, MARIA EUGENIA en que OP y boletin se menciona?",
    "k_final": 3
  }' | jq
```

**Respuesta incluye:**
- `answer`: Respuesta generada por LLM
- `verification`: VerificaciÃ³n de hallazgos (âœ… respaldada / âš ï¸ parcial / âŒ contradictoria)
- `results`: Top-k chunks con scores hÃ­bridos (BM25 + vector + RRF)
- `performance`: Desglose de latencias por componente (ms)
- `cost`: Costo estimado en USD (tokens OpenAI)

**Ejemplo de performance:**
```json
{
  "total_time": 11.945,
  "breakdown": {
    "bm25_time": 0.014,
    "vector_time": 1.132,
    "fusion_time": 0.0,
    "llm_summary_time": 3.99,
    "llm_answer_time": 2.745,
    "llm_verification_time": 3.967
  },
  "cost": { "total_usd": 0.000704 }
}
```

### Ver metadatos de un resultado

```bash
curl -s -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "expediente 123", "k_final": 1}' \
  | jq '.results[0].metadata'
```

**Ejemplo de metadata:**
```json
{
  "chunk_id": "22044_2025-10-02::p1::2",
  "source": "boletines/2025/22044_2025-10-02.pdf",
  "boletin": "22044",
  "fecha": "2025-10-02",
  "op": "100128767",
  "ndjson_key": "rag/chunks_op/2025/22044_2025-10-02.ndjson"
}
```

### Otros endpoints

- `POST /vector/query` - Consulta directa al Ã­ndice vectorial (sin BM25)
- `GET /eval/precision` - MÃ©tricas de precisiÃ³n contra qrels
- `GET /eval/recall` - MÃ©tricas de recall

---

## ğŸ’» UI con Streamlit

Acceder a **http://localhost:8501**

La interfaz permite:
- Hacer preguntas en lenguaje natural
- Ver chunks recuperados con scores
- Inspeccionar metadatos (boletÃ­n, OP, fecha)
- Visualizar tiempos de respuesta

> La variable `RAG_API_URL` en el container apunta a `http://rag_api:8000` (DNS interno del compose).

---

## ğŸ“Š Jupyter Notebook (Desarrollo)

Acceder a **http://localhost:8888** (sin token)

El servicio `rag_notebook` tiene acceso a:
- CÃ³digo de `fastapi_app/` (modo read-only)
- MÃ³dulos de `plugins/tasks/` (Ã­ndices, fusiÃ³n, mÃ©tricas)
- Variables de entorno de S3, Pinecone y OpenAI

Ãštil para:
- Prototipado de chunking strategies
- Pruebas de fusiÃ³n RRF
- AnÃ¡lisis de embeddings

---

## ğŸ—„ Estructura de Datos en MinIO

**Bucket**: `respaldo2`

```
rag/
â”œâ”€ models/
â”‚  â””â”€ 2025/
â”‚     â””â”€ bm25.pkl                          # Ãndice BM25 serializado (pickle)
â”‚
â”œâ”€ chunks_op/                              # ğŸ“ NDJSON generado por pipeline
â”‚  â””â”€ 2025/
â”‚     â”œâ”€ 22043_2025-10-01.ndjson           # Chunks del boletÃ­n 22043
â”‚     â”œâ”€ 22044_2025-10-02.ndjson
â”‚     â””â”€ ...
â”‚
â”œâ”€ chunks_op_curated/                      # ğŸ“ NDJSON corregido manualmente
â”‚  â””â”€ 2025/
â”‚
â”œâ”€ text_op/                                # ğŸ“ Texto crudo por OP
â”‚  â””â”€ 2025/
â”‚     â””â”€ 22047_OP100129085_2025-10-07.txt
â”‚
â”œâ”€ text_op_meta/                           # ğŸ“ Metadatos por OP (JSON)
â”‚  â””â”€ 2025/
â”‚     â””â”€ 22047_OP100129085_2025-10-07.json
â”‚
â”œâ”€ fusion/                                 # ğŸ“ Resultados de bÃºsquedas RRF
â”‚  â””â”€ 2025/
â”‚     â””â”€ fusion_<query>_<timestamp>.json
â”‚
â”œâ”€ metrics/                                # ğŸ“ Evaluaciones (AP@k, nDCG@k, MRR)
â”‚  â””â”€ 2025/
â”‚     â””â”€ fusion_eval_<query>.json
â”‚
â””â”€ qrels/                                  # ğŸ“ Ground truth para evaluaciÃ³n
   â””â”€ 2025/
      â””â”€ qrels.csv
```

---

## ğŸ›  Troubleshooting

### Error: "No module named 'tasks'"

**Causa**: PYTHONPATH no incluye `plugins/` o `fastapi_app/`

**SoluciÃ³n**: Verificar en `docker-compose.yaml` que los volÃºmenes estÃ©n montados:
```yaml
volumes:
  - ./plugins/tasks:/code/tasks
  - ./fastapi_app:/code/fastapi_app
environment:
  PYTHONPATH: /code
```

### Error: "âš ï¸ OPENAI_API_KEY no configurada"

**SoluciÃ³n**: Agregar en `.env`:
```bash
OPENAI_API_KEY=sk-proj-XXXXXXXXXXXXXXXX
```

### Error: Pinecone "dimension mismatch"

**Causa**: El Ã­ndice fue creado con una dimensiÃ³n diferente a 384

**SoluciÃ³n**: 
1. Eliminar el Ã­ndice en Pinecone
2. Crear uno nuevo con `dimension=384`
3. Ejecutar el pipeline de ingesta nuevamente

### No se encuentran resultados en la API

**DiagnÃ³stico**:
```bash
# Verificar que exista bm25.pkl
aws --endpoint-url http://localhost:9000 s3 ls s3://respaldo2/rag/models/2025/

# Verificar que existan chunks
aws --endpoint-url http://localhost:9000 s3 ls s3://respaldo2/rag/chunks_op/2025/
```

**SoluciÃ³n**: Ejecutar el DAG `Ingestion_pipeline` en Airflow.

### ConexiÃ³n rechazada desde la API a MinIO

**Causa**: La API intenta conectarse a `localhost:9000` en lugar de `minio:9000`

**SoluciÃ³n**: En `.env`, verificar:
```bash
S3_ENDPOINT_URL=http://minio:9000  # DNS interno del compose
```

### Ver logs de un servicio

```bash
# Logs en tiempo real
docker compose logs -f rag_api
docker compose logs -f airflow-scheduler

# Logs de todos los servicios
docker compose logs --tail=100
```

### Reiniciar solo un servicio

```bash
docker compose restart rag_api
docker compose up -d --build rag_api  # con rebuild
```

### Limpiar todo y empezar de cero

```bash
docker compose down -v  # elimina volÃºmenes
docker compose up -d --build
```

---

## ğŸ“¦ Flujo de Trabajo TÃ­pico

1. **Desarrollo**:
   - Prototipar en `notebooks/pipeline_rag.ipynb` (Jupyter)
   - Refinar chunking en `plugins/tasks/chunk_*.py`
   - Probar fusiÃ³n RRF localmente

2. **Ingesta**:
   - Subir PDFs a MinIO (manual o automatizado)
   - Ejecutar DAG `Ingestion_pipeline` en Airflow
   - Validar que se generen `bm25.pkl` y `*.ndjson`

3. **Consulta**:
   - Usar Streamlit para pruebas interactivas
   - Usar FastAPI directamente para integraciones
   - Monitorear `performance` y ajustar parÃ¡metros

4. **EvaluaciÃ³n**:
   - Crear `qrels.csv` con relevancias ground truth
   - Ejecutar tasks de evaluaciÃ³n (`eval_fusion_task.py`)
   - Revisar mÃ©tricas en `rag/metrics/`

---

## ğŸ§ª Comandos Ãštiles

```bash
# Estado de los servicios
docker compose ps

# Logs de un servicio especÃ­fico
docker compose logs -f streamlit

# Ejecutar comando en un container
docker compose exec rag_api bash

# Ver uso de recursos
docker stats

# Limpiar imÃ¡genes no usadas
docker image prune -a

# Backup de la DB de Airflow
docker compose exec postgres pg_dump -U airflow airflow > airflow_backup.sql
```

---

## ğŸ“ Notas Adicionales

- **Seguridad**: Este setup es para desarrollo local. En producciÃ³n, cambiar credenciales, habilitar SSL y usar secrets.
- **Escalado**: Para mayor throughput, aumentar `airflow-worker` replicas o cambiar a KubernetesExecutor.
- **Costos**: Monitorear `cost.total_usd` en las respuestas de la API para estimar gastos en OpenAI.
- **MLflow**: El tracking server guarda experimentos en `./mlflow/`. Ãštil para versionar modelos y parÃ¡metros.

---

## ğŸ¤ Contribuciones

Este proyecto es parte del **Trabajo Final de NLP (UBA)**.  
Para reportar issues o sugerir mejoras, abrir un issue en el repositorio.

---

## ğŸ“„ Licencia

Ver `LICENSE` en el repositorio.